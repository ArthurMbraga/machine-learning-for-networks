{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural-networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMHLlQ34Lh6K8O2cSlqoM7Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreaaraldo/machine-learning-for-networks/blob/master/04.neural_networks/neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHY0Omw3e1eb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "33549f18-8eb9-402b-84a1-18898e606304"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "# We will use one implementation of Keras called tensorflow.keras.\n",
        "# There are also other implementations of Keras\n",
        "# See Ch.10 of [Ge19]\n",
        "# from tensorflow import keras\n",
        "\n",
        "import keras\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.utils import plot_model\n",
        "\n",
        "\n",
        "# The following library is to plot the loss during training\n",
        "# https://github.com/stared/livelossplot\n",
        "!pip install git+git://github.com/stared/livelossplot.git\n",
        "from livelossplot.keras import PlotLossesCallback\n",
        "\n",
        "\n",
        "# Import the visualization library I prepared for you\n",
        "! wget https://raw.githubusercontent.com/andreaaraldo/machine-learning-for-networks/master/course_library/visualization.py\n",
        "from visualization import plot_conf_mat"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/stared/livelossplot.git\n",
            "  Cloning git://github.com/stared/livelossplot.git to /tmp/pip-req-build-e243mkeq\n",
            "  Running command git clone -q git://github.com/stared/livelossplot.git /tmp/pip-req-build-e243mkeq\n",
            "Requirement already satisfied (use --upgrade to upgrade): livelossplot==0.4.2 from git+git://github.com/stared/livelossplot.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from livelossplot==0.4.2) (3.1.3)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from livelossplot==0.4.2) (5.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.4.2) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.4.2) (2.4.6)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.4.2) (1.17.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.4.2) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.4.2) (2.6.1)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.2) (4.5.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.2) (5.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.2) (4.3.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.2) (5.0.4)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.2) (5.3.4)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.2) (0.2.0)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.2) (0.8.3)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.2) (4.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.2) (2.11.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.2) (4.6.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->livelossplot==0.4.2) (45.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->livelossplot==0.4.2) (1.12.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.4.2) (0.6.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.4.2) (0.4.4)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.4.2) (2.1.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.4.2) (1.4.2)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.4.2) (0.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.4.2) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.4.2) (3.1.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook->livelossplot==0.4.2) (4.4.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook->livelossplot==0.4.2) (2.6.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->livelossplot==0.4.2) (17.0.0)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->livelossplot==0.4.2) (0.6.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->notebook->livelossplot==0.4.2) (5.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->livelossplot==0.4.2) (1.1.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook->livelossplot==0.4.2) (0.5.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.4.2) (1.0.18)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.4.2) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.4.2) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.4.2) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->notebook->livelossplot==0.4.2) (0.1.8)\n",
            "Building wheels for collected packages: livelossplot\n",
            "  Building wheel for livelossplot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for livelossplot: filename=livelossplot-0.4.2-cp36-none-any.whl size=12661 sha256=894547009b8670bffa12016d27d2e84cc39c615fc593ba539659fffab696789d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4ps0c02k/wheels/77/01/ea/cef3581d9c77ece0fd685cc3eb1cd92dc68d8117b361ac65dc\n",
            "Successfully built livelossplot\n",
            "--2020-03-16 14:27:27--  https://raw.githubusercontent.com/andreaaraldo/machine-learning-for-networks/master/course_library/visualization.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3087 (3.0K) [text/plain]\n",
            "Saving to: ‘visualization.py.2’\n",
            "\n",
            "visualization.py.2  100%[===================>]   3.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-03-16 14:27:27 (82.7 MB/s) - ‘visualization.py.2’ saved [3087/3087]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/content/visualization.py\"\u001b[0;36m, line \u001b[0;32m59\u001b[0m\n\u001b[0;31m    def plot_conf_mat(y_true, y_pred, classes,\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAyrD98bGJDl",
        "colab_type": "text"
      },
      "source": [
        "# Use case description\n",
        "\n",
        "The use case is from [KhConf19].\n",
        "\n",
        "\n",
        "**Goal** Estimate available bandwidth in a network via **passive measures**.\n",
        "\n",
        "More precisely:\n",
        "_Estimate the capacity available to a TCP flow_ (sharing links with other flows) observing\n",
        "* The time gaps between segments sent $g_{\\text{in}}$\n",
        "* The gaps between acks $g_\\text{ack}$\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/andreaaraldo/machine-learning-for-networks/master/04.neural_networks/img/ack.png)\n",
        "\\[Figure from [KhThesis19] \\]\n",
        "\n",
        "\n",
        "The auhtors set up the following testbed:\n",
        "\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/andreaaraldo/machine-learning-for-networks/master/04.neural_networks/img/testbed.png)\n",
        "\n",
        "\n",
        "Measures are collected in the **Video Receiver**. All the other machines just produce cross-traffic.\n",
        "\n",
        "Measures are recorded via an Endace Data Acquisition and Generation (DAG) card, which timestamp all packets in an extremely precise way.\n",
        "\n",
        "![alt text](https://www.endace.com/assets/images/products/DAG%209.5G4F_angled_small.png)\n",
        "\n",
        "([Producer website](https://www.endace.com/endace-high-speed-packet-capture-solutions/oem/dag/))\n",
        "\n",
        "**Why**: Knowing the available bandwidth, video streaming clients can properly choose the quality level to request."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXQAcMSZW4n_",
        "colab_type": "text"
      },
      "source": [
        "# Traces\n",
        "\n",
        "The description of the dataset can be found in the Appendix of Khangura's [PhD thesis](https://www.repo.uni-hannover.de/bitstream/handle/123456789/9219/Khangura_Sukhpreet_PhD_Thesis.pdf?sequence=3&isAllowed=y)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u-6liSdGH62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://www.ikt.uni-hannover.de/fileadmin/institut/Forschung/BandwidthEstimationTraces/BandwidthEstimationTraces.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blbuQi5dWFRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! unzip -o -q BandwidthEstimationTraces.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFiAO1GMWmR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls BandwidthEstimationTraces"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRvAQC70cisA",
        "colab_type": "text"
      },
      "source": [
        "Training and test datasets are separated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvKB2SxTWoYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls BandwidthEstimationTraces/training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FOIPBZocvZo",
        "colab_type": "text"
      },
      "source": [
        "For simplicity, we will just consider the case with a single link between client and server, of total capacity C=100 Mbps (Ethernet level)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W306F9Azc826",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls BandwidthEstimationTraces/training/SingleLinkCapacity100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPBsb5Axdd2z",
        "colab_type": "text"
      },
      "source": [
        "There are three sets of traces:\n",
        "* With cross traffic rate $\\lambda$=25 Mbps\n",
        "* With cross traffic rate $\\lambda$=50 Mbps\n",
        "* With cross traffic rate $\\lambda$=75 Mbps\n",
        "\n",
        "All rates are intended at the Ethernet level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P210sSI9dDJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls BandwidthEstimationTraces/training/SingleLinkCapacity100/75_et_100_C_5_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTeZj2Z3eJwV",
        "colab_type": "text"
      },
      "source": [
        "Every experiment is repeated 100 times.\n",
        "\n",
        "Let's check the trace of one experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRwGpDEHeMkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = \"BandwidthEstimationTraces/training/SingleLinkCapacity100/75_et_100_C_5_delta/75_et_100_C_5_delta_33.csv\"\n",
        "df = pd.read_csv(filename)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgpozAfk8YZv",
        "colab_type": "text"
      },
      "source": [
        "The header is not a sample. It just tells us the scenario, i.e. total channel capacity (Mbps) and available bandwith (Mbps).\n",
        "\n",
        "The columns are:\n",
        "* $g_\\text{in} / g_\\text{ack}$\n",
        "* Some sort of time stamp that we will ignore (not well described in the dataset)\n",
        "\n",
        "Let's rename the columns to avoid ambiguity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj41JABXM355",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.columns=['gap_ratio', 'timestamp']\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv8RhycEQsDM",
        "colab_type": "text"
      },
      "source": [
        "# Feature engineering\n",
        "\n",
        "Each experiment will be a sample.\n",
        "\n",
        "The features of a sample are the elements of the histogram of the first column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFbGLRg6NfSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(df['gap_ratio'], density=True)\n",
        "\n",
        "# density:  True garantees that the area is 1 (such that hist approximates a \n",
        "#                 probability densitplt.hist(x)y function)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cncq7wY4PkDR",
        "colab_type": "text"
      },
      "source": [
        "However, the sequence of bins is chosen automatically by `matplotlib` and may change from an experiment to another.\n",
        "\n",
        "We need instead to describe all the experiments with a uniform set of features => The sequence of bins must be the same for all the experiments.\n",
        "\n",
        "To do so:\n",
        "* Take the min and max gap from all the experiments\n",
        "* Divide the [min,max] interval uniformly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcGA9B5LQaSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_gap = float('inf')\n",
        "max_gap = 0\n",
        "for cross_traf in ['25', '50', '75']:\n",
        "  for exp_id in range(1,101):\n",
        "    filename = \"BandwidthEstimationTraces/training/SingleLinkCapacity100/\" +\\\n",
        "                cross_traf + \"_et_100_C_5_delta/\"+\\\n",
        "                cross_traf + \"_et_100_C_5_delta_\"+str(exp_id)+\".csv\"\n",
        "\n",
        "    # print('Checking ',filename)\n",
        "    df = pd.read_csv(filename)\n",
        "    df.columns=['gap_ratio', 'timestamp']\n",
        "    trace_min = min( df['gap_ratio'] )\n",
        "    trace_max = max( df['gap_ratio'] )\n",
        "    min_gap = min ( [ min_gap, trace_min ] )\n",
        "    max_gap = max ( [ max_gap, trace_max ] )\n",
        "\n",
        "print('min_gap:', min_gap, ' max_gap:',max_gap)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS7y7cT-XmpQ",
        "colab_type": "text"
      },
      "source": [
        "Let's create the bins that we will use for all experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPnUK2tiXuEB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = 8 # num of bins\n",
        "\n",
        "bin_size = (max_gap-min_gap)/N \n",
        "bins = [min_gap + i * bin_size for i in range(0, N+1)]\n",
        "bins"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atGuyYNKYR-1",
        "colab_type": "text"
      },
      "source": [
        "Just as a visual check, let's plot again the previous histogram with these new bins"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWkFTXN0YWH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = \"BandwidthEstimationTraces/training/SingleLinkCapacity100/75_et_100_C_5_delta/75_et_100_C_5_delta_33.csv\"\n",
        "df = pd.read_csv(filename)\n",
        "df.columns=['gap_ratio', 'timestamp']\n",
        "hist_values, bins, bars = plt.hist( df['gap_ratio'], density=True, bins=bins)\n",
        "print(hist_values )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyjd51oBcD8n",
        "colab_type": "text"
      },
      "source": [
        "Let's obtain the histogram for all the experiments. For each experiment, we construct a feature vector:\n",
        "\n",
        "  `[bin1_freq, bin2_freq, ...., binN_freq]`\n",
        "\n",
        "and a label `avail_bandwidth`.\n",
        "\n",
        "We write these operations in a function `construct_dataset` that we will then use for the training and the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCEHvKgTaGjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def from_str_to_num(the_str):\n",
        "  num = 0 \n",
        "  if the_str=='25': num=0\n",
        "  elif the_str=='50': num=1\n",
        "  elif the_str=='75': num=2\n",
        "  else: raise ValueError('Value ', the_str, ' is not valid')\n",
        "  return num;\n",
        "\n",
        "\n",
        "def construct_dataset(filepath, bins):\n",
        "\n",
        "  X = np.empty((0,N), int)\n",
        "  y = np.empty((0,1), int)\n",
        "\n",
        "  for cross_traf in ['25', '50', '75']:\n",
        "    for exp_id in range(1,101):\n",
        "      filename = filepath +\\\n",
        "                  cross_traf + \"_et_100_C_5_delta/\"+\\\n",
        "                  cross_traf + \"_et_100_C_5_delta_\"+str(exp_id)+\".csv\"\n",
        "\n",
        "      df = pd.read_csv(filename)\n",
        "\n",
        "      # Observe that for each experiment, the available bandwidth corresponds to the\n",
        "      # name of the second column\n",
        "      avail_band = from_str_to_num( df.columns[1] )\n",
        "\n",
        "\n",
        "      df.columns=['gap_ratio', 'timestamp']\n",
        "\n",
        "      hist_values, bins, bars = plt.hist( df['gap_ratio'], density=True, bins=bins)\n",
        "\n",
        "      X = np.vstack(( X, hist_values) )\n",
        "      y = np.vstack(( y, avail_band) )\n",
        "\n",
        "  y = y.reshape(-1,1)\n",
        "  return X,y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6Ekap9q-_ia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, y_train = construct_dataset(\n",
        "    \"BandwidthEstimationTraces/training/SingleLinkCapacity100/\", \n",
        "    bins)\n",
        "X_test, y_test = construct_dataset(\n",
        "    \"BandwidthEstimationTraces/testing/SingleLinkCapacity100/\",\n",
        "    bins)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STsxNvnVB-ZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('X_train', X_train[0:6, :] )\n",
        "print('y_train',y_train[0:6] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K03VxUJ1idJp",
        "colab_type": "text"
      },
      "source": [
        "With NN is important to **scale** the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CpWt6qKivif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print('X_train_scaled', X_train_scaled[0:6, :] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_iC8vrtk3uE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsDxd9KuCnzg",
        "colab_type": "text"
      },
      "source": [
        "# Build a NN model\n",
        "\n",
        "To train faster, change the runtime to GPU.\n",
        "\n",
        "Now, let's build a NN architecture. The size of each sample is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLRGAHB2oPJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_size = X_train_scaled.shape[1]\n",
        "print('The sample size is ', sample_size, \n",
        "      ', which should correspond to the number of bins ', N)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0zcfeTEC0KF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_of_classes = 3\n",
        "\n",
        "model = Sequential([\n",
        "  Dense(4, input_dim=sample_size, activation='relu' ),\n",
        "  Dense(num_of_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# The first layer has 4 neurons and take 8 input values.\n",
        "# In the first layer, you always need to specify the input_dim\n",
        "#\n",
        "# Note that each layer adds implicitly a bias term (we do not need to care about \n",
        "# it)\n",
        "#\n",
        "# The last layer is a softmax, since we are doing classification\n",
        "\n",
        "\n",
        "model.summary()\n",
        "plot_model(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2WOD4aNt3HA",
        "colab_type": "text"
      },
      "source": [
        "`None` means that the batch size can be any value.\n",
        "\n",
        "Compile the model, i.e., decide the loss function to minimize, the optimization function and the metrics to show"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0eA8qTowf16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
        "              optimizer=keras.optimizers.SGD(lr=0.01) ,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0SfFwbExxs9",
        "colab_type": "text"
      },
      "source": [
        "From Ch.10 of [Ge19]:\n",
        "\n",
        "Loss function:\n",
        "* Use `sparse_categorical_cross entropy` when there is just one target value (as in our case). \n",
        "* If target were one-hot encoded, e.g. [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], then use `categorical_crossentropy` instead. \n",
        "* In binary classification then use `sigmoid` activation function in the output layer instead of the `softmax` and `binary_crossentropy` loss.\n",
        "\n",
        "Optimizer:\n",
        "* We are using Stochastic Gradient Descent with learning rate $\\eta=0.01$\n",
        "\n",
        "Metrics:\n",
        "* We are asking Keras to show at each epoch the accuracy. Note that the accuracy value is ignored during training. This metric is just visualized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AZY1xtwl0jq",
        "colab_type": "text"
      },
      "source": [
        "# Training\n",
        "\n",
        "To train faster, change the runtime to GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPT7aAEQ0d_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_losses = PlotLossesCallback()\n",
        "\n",
        "if False:\n",
        "  history = model.fit(X_train_scaled, y_train, epochs=1000, \n",
        "                    callbacks = [plot_losses] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnG48l8XESqF",
        "colab_type": "text"
      },
      "source": [
        "To avoid spending time for training, I add directly the results\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/andreaaraldo/machine-learning-for-networks/master/04.neural_networks/img/bad_training.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IADgsSbyEcUh",
        "colab_type": "text"
      },
      "source": [
        "Our NN is a disaster! Let's anyway check the performance on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u27rGU0EqJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test_scaled)\n",
        "y_pred[0:5,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvWVnm5pG505",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict_classes(X_test_scaled)\n",
        "y_pred[0:5]\n",
        "\n",
        "plot_confusion_matrix(model, X_test_scaled, y_test, normalize='true')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u0cMK1JDKLH",
        "colab_type": "text"
      },
      "source": [
        "# Limitation of the work\n",
        "\n",
        "* Only a finite set of available bandwidth values are used (25, 50, 75 Mbps). In reality, any value can occur => Need to extend the test and validation test with random bandwidth values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VYbiDvHxLtU",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "\n",
        "* [Ge19] Geron, Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2019, O'Reilly\n",
        "* [KhConf19] Khangura, S. K. (2019). Neural Network-based Available Bandwidth Estimation from TCP Sender-side Measurements. In IEEE/IFIP PEMWN.\n",
        "* [KhThesis19] Khangura, S. K. (2019). Machine Learning-based Available Bandwidth Estimation. Leibniz University."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfLC0FZpGl13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}