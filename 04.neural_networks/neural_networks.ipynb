{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural-networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhIZPY7fSdWUDHi11iTZuw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreaaraldo/machine-learning-for-networks/blob/master/04.neural_networks/neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHY0Omw3e1eb",
        "colab_type": "code",
        "outputId": "0a4bf3cc-1a10-4669-e318-e0e842d7725e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.utils import plot_model\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "\n",
        "\n",
        "\n",
        "!pip install category_encoders\n",
        "import category_encoders as ce\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# The following library is to plot the loss during training\n",
        "# https://github.com/stared/livelossplot\n",
        "!pip install git+git://github.com/stared/livelossplot.git\n",
        "from livelossplot.keras import PlotLossesCallback\n",
        "\n",
        "\n",
        "# Import the visualization library I prepared for you\n",
        "! wget https://raw.githubusercontent.com/andreaaraldo/machine-learning-for-networks/master/course_library/visualization.py\n",
        "from visualization import plot_conf_mat\n",
        "\n",
        "\n",
        "# The following is to be able to mount Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "import os\n",
        "from os.path import isfile"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting category_encoders\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/52/c54191ad3782de633ea3d6ee3bb2837bda0cf3bc97644bb6375cf14150a0/category_encoders-2.1.0-py2.py3-none-any.whl (100kB)\n",
            "\r\u001b[K     |███▎                            | 10kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20kB 31.5MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 30kB 37.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40kB 41.2MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 51kB 37.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 61kB 39.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 71kB 31.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 81kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 92kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 12.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.10.2)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.18.1)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.4.1)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.25.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.1->category_encoders) (1.12.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Installing collected packages: category-encoders\n",
            "Successfully installed category-encoders-2.1.0\n",
            "Collecting git+git://github.com/stared/livelossplot.git\n",
            "  Cloning git://github.com/stared/livelossplot.git to /tmp/pip-req-build-4fp2dii9\n",
            "  Running command git clone -q git://github.com/stared/livelossplot.git /tmp/pip-req-build-4fp2dii9\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from livelossplot==0.4.2) (3.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from livelossplot==0.4.2) (5.2.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.4.2) (1.18.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.4.2) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.4.2) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.4.2) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.4.2) (2.4.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.2) (4.6.1)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.2) (4.5.3)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.2) (0.8.3)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.2) (4.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.2) (2.11.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.2) (4.6.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.2) (5.3.4)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.2) (5.6.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.2) (5.0.4)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.2) (0.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->livelossplot==0.4.2) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->livelossplot==0.4.2) (45.2.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->notebook->livelossplot==0.4.2) (5.5.0)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->livelossplot==0.4.2) (0.6.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook->livelossplot==0.4.2) (4.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->livelossplot==0.4.2) (1.1.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->livelossplot==0.4.2) (17.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.4.2) (0.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.4.2) (3.1.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.4.2) (1.4.2)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.4.2) (0.4.4)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.4.2) (2.1.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.4.2) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.4.2) (0.3)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook->livelossplot==0.4.2) (2.6.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.4.2) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.4.2) (1.0.18)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.4.2) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.4.2) (0.7.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook->livelossplot==0.4.2) (0.5.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->notebook->livelossplot==0.4.2) (0.1.8)\n",
            "Building wheels for collected packages: livelossplot\n",
            "  Building wheel for livelossplot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for livelossplot: filename=livelossplot-0.4.2-cp36-none-any.whl size=12661 sha256=1e6dcd739c76ada15ff1003e3e34a4242046b8d2a706302506f6bb740b45b8b3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fi5a75ic/wheels/77/01/ea/cef3581d9c77ece0fd685cc3eb1cd92dc68d8117b361ac65dc\n",
            "Successfully built livelossplot\n",
            "Installing collected packages: livelossplot\n",
            "Successfully installed livelossplot-0.4.2\n",
            "--2020-03-18 22:42:58--  https://raw.githubusercontent.com/andreaaraldo/machine-learning-for-networks/master/course_library/visualization.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4406 (4.3K) [text/plain]\n",
            "Saving to: ‘visualization.py’\n",
            "\n",
            "visualization.py    100%[===================>]   4.30K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-03-18 22:42:58 (101 MB/s) - ‘visualization.py’ saved [4406/4406]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6IQYJ3N_KSM",
        "colab_type": "text"
      },
      "source": [
        "### Mount your Google Drive\n",
        "\n",
        "Training a neural network may take long time and you don't want to do it every time. Once you've trained a model, it is better to save it, so that you can use it immediately for prediction next times.\n",
        "\n",
        "Unfortunately, the storage on Google is reset every time. You need to store all persistent data in you Google Drive.\n",
        "\n",
        "You need therefore to mount your Google Drive, which you will use later in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blP3nm_stBVH",
        "colab_type": "code",
        "outputId": "e9aa45fa-9451-46eb-8073-09827052595f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "mount_point = '/content/gdrive' # Always the same, don't change it\n",
        "drive.mount(mount_point, force_remount=True)\n",
        "drive_path = mount_point + '/My Drive/' # Always the same, don't change it\n",
        "my_path = drive_path + \\\n",
        "  'tsp/teaching/data-science-for-networks/img-from-code/04.neural-networks/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAyrD98bGJDl",
        "colab_type": "text"
      },
      "source": [
        "# Use case description\n",
        "\n",
        "The use case is from [KhConf19].\n",
        "\n",
        "\n",
        "**Goal** Estimate available bandwidth in a network via **passive measures**.\n",
        "\n",
        "More precisely:\n",
        "_Estimate the capacity available to a TCP flow_ (sharing links with other flows) observing\n",
        "* The time gaps between segments sent $g_{\\text{in}}$\n",
        "* The gaps between acks $g_\\text{ack}$\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/andreaaraldo/machine-learning-for-networks/master/04.neural_networks/img/ack.png)\n",
        "\\[Figure from [KhThesis19] \\]\n",
        "\n",
        "\n",
        "The auhtors set up the following testbed:\n",
        "\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/andreaaraldo/machine-learning-for-networks/master/04.neural_networks/img/testbed.png)\n",
        "\n",
        "\n",
        "Measures are collected in the **Video Receiver**. All the other machines just produce cross-traffic.\n",
        "\n",
        "Measures are recorded via an Endace Data Acquisition and Generation (DAG) card, which timestamp all packets in an extremely precise way.\n",
        "\n",
        "![alt text](https://www.endace.com/assets/images/products/DAG%209.5G4F_angled_small.png)\n",
        "\n",
        "([Producer website](https://www.endace.com/endace-high-speed-packet-capture-solutions/oem/dag/))\n",
        "\n",
        "**Why**: Knowing the available bandwidth, video streaming clients can properly choose the quality level to request."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXQAcMSZW4n_",
        "colab_type": "text"
      },
      "source": [
        "# Traces\n",
        "\n",
        "The description of the dataset can be found in the Appendix of Khangura's [PhD thesis](https://www.repo.uni-hannover.de/bitstream/handle/123456789/9219/Khangura_Sukhpreet_PhD_Thesis.pdf?sequence=3&isAllowed=y)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u-6liSdGH62",
        "colab_type": "code",
        "outputId": "4e06f2e4-878d-4f85-9afc-7f1787a91eb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "!wget https://www.ikt.uni-hannover.de/fileadmin/institut/Forschung/BandwidthEstimationTraces/BandwidthEstimationTraces.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-18 22:43:25--  https://www.ikt.uni-hannover.de/fileadmin/institut/Forschung/BandwidthEstimationTraces/BandwidthEstimationTraces.zip\n",
            "Resolving www.ikt.uni-hannover.de (www.ikt.uni-hannover.de)... 130.75.2.72\n",
            "Connecting to www.ikt.uni-hannover.de (www.ikt.uni-hannover.de)|130.75.2.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 941822 (920K) [application/zip]\n",
            "Saving to: ‘BandwidthEstimationTraces.zip’\n",
            "\n",
            "BandwidthEstimation 100%[===================>] 919.75K   191KB/s    in 4.8s    \n",
            "\n",
            "2020-03-18 22:43:32 (191 KB/s) - ‘BandwidthEstimationTraces.zip’ saved [941822/941822]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blbuQi5dWFRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! unzip -o -q BandwidthEstimationTraces.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFiAO1GMWmR8",
        "colab_type": "code",
        "outputId": "8829ad15-95f5-49ad-c485-9380d6fbdf4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls BandwidthEstimationTraces"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing  training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRvAQC70cisA",
        "colab_type": "text"
      },
      "source": [
        "Training and test datasets are separated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvKB2SxTWoYI",
        "colab_type": "code",
        "outputId": "08bd8efc-ad55-4912-feb1-9615f67bf2a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "! ls BandwidthEstimationTraces/training"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MultiLinkCapacity100   TightLinkafterBottleneckLink\n",
            "SingleLinkCapacity100  TightLinkbeforeBottleneckLink\n",
            "SingleLinkCapacity50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FOIPBZocvZo",
        "colab_type": "text"
      },
      "source": [
        "For simplicity, we will just consider the case with a single link between client and server."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W306F9Azc826",
        "colab_type": "code",
        "outputId": "3c972149-103f-4073-c8f5-76e42ca18c01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! ls BandwidthEstimationTraces/training/SingleLinkCapacity100"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25_et_100_C_5_delta  50_et_100_C_5_delta  75_et_100_C_5_delta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPBsb5Axdd2z",
        "colab_type": "text"
      },
      "source": [
        "There are three sets of traces:\n",
        "* With cross traffic rate $\\lambda$=25 Mbps\n",
        "* With cross traffic rate $\\lambda$=50 Mbps\n",
        "* With cross traffic rate $\\lambda$=75 Mbps\n",
        "\n",
        "All rates are intended at the Ethernet level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P210sSI9dDJv",
        "colab_type": "code",
        "outputId": "01728f56-5591-4f1c-e5a0-a416aa9d0f02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        }
      },
      "source": [
        "! ls BandwidthEstimationTraces/training/SingleLinkCapacity100/75_et_100_C_5_delta"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "75_et_100_C_5_delta_100.csv  75_et_100_C_5_delta_55.csv\n",
            "75_et_100_C_5_delta_10.csv   75_et_100_C_5_delta_56.csv\n",
            "75_et_100_C_5_delta_11.csv   75_et_100_C_5_delta_57.csv\n",
            "75_et_100_C_5_delta_12.csv   75_et_100_C_5_delta_58.csv\n",
            "75_et_100_C_5_delta_13.csv   75_et_100_C_5_delta_59.csv\n",
            "75_et_100_C_5_delta_14.csv   75_et_100_C_5_delta_5.csv\n",
            "75_et_100_C_5_delta_15.csv   75_et_100_C_5_delta_60.csv\n",
            "75_et_100_C_5_delta_16.csv   75_et_100_C_5_delta_61.csv\n",
            "75_et_100_C_5_delta_17.csv   75_et_100_C_5_delta_62.csv\n",
            "75_et_100_C_5_delta_18.csv   75_et_100_C_5_delta_63.csv\n",
            "75_et_100_C_5_delta_19.csv   75_et_100_C_5_delta_64.csv\n",
            "75_et_100_C_5_delta_1.csv    75_et_100_C_5_delta_65.csv\n",
            "75_et_100_C_5_delta_20.csv   75_et_100_C_5_delta_66.csv\n",
            "75_et_100_C_5_delta_21.csv   75_et_100_C_5_delta_67.csv\n",
            "75_et_100_C_5_delta_22.csv   75_et_100_C_5_delta_68.csv\n",
            "75_et_100_C_5_delta_23.csv   75_et_100_C_5_delta_69.csv\n",
            "75_et_100_C_5_delta_24.csv   75_et_100_C_5_delta_6.csv\n",
            "75_et_100_C_5_delta_25.csv   75_et_100_C_5_delta_70.csv\n",
            "75_et_100_C_5_delta_26.csv   75_et_100_C_5_delta_71.csv\n",
            "75_et_100_C_5_delta_27.csv   75_et_100_C_5_delta_72.csv\n",
            "75_et_100_C_5_delta_28.csv   75_et_100_C_5_delta_73.csv\n",
            "75_et_100_C_5_delta_29.csv   75_et_100_C_5_delta_74.csv\n",
            "75_et_100_C_5_delta_2.csv    75_et_100_C_5_delta_75.csv\n",
            "75_et_100_C_5_delta_30.csv   75_et_100_C_5_delta_76.csv\n",
            "75_et_100_C_5_delta_31.csv   75_et_100_C_5_delta_77.csv\n",
            "75_et_100_C_5_delta_32.csv   75_et_100_C_5_delta_78.csv\n",
            "75_et_100_C_5_delta_33.csv   75_et_100_C_5_delta_79.csv\n",
            "75_et_100_C_5_delta_34.csv   75_et_100_C_5_delta_7.csv\n",
            "75_et_100_C_5_delta_35.csv   75_et_100_C_5_delta_80.csv\n",
            "75_et_100_C_5_delta_36.csv   75_et_100_C_5_delta_81.csv\n",
            "75_et_100_C_5_delta_37.csv   75_et_100_C_5_delta_82.csv\n",
            "75_et_100_C_5_delta_38.csv   75_et_100_C_5_delta_83.csv\n",
            "75_et_100_C_5_delta_39.csv   75_et_100_C_5_delta_84.csv\n",
            "75_et_100_C_5_delta_3.csv    75_et_100_C_5_delta_85.csv\n",
            "75_et_100_C_5_delta_40.csv   75_et_100_C_5_delta_86.csv\n",
            "75_et_100_C_5_delta_41.csv   75_et_100_C_5_delta_87.csv\n",
            "75_et_100_C_5_delta_42.csv   75_et_100_C_5_delta_88.csv\n",
            "75_et_100_C_5_delta_43.csv   75_et_100_C_5_delta_89.csv\n",
            "75_et_100_C_5_delta_44.csv   75_et_100_C_5_delta_8.csv\n",
            "75_et_100_C_5_delta_45.csv   75_et_100_C_5_delta_90.csv\n",
            "75_et_100_C_5_delta_46.csv   75_et_100_C_5_delta_91.csv\n",
            "75_et_100_C_5_delta_47.csv   75_et_100_C_5_delta_92.csv\n",
            "75_et_100_C_5_delta_48.csv   75_et_100_C_5_delta_93.csv\n",
            "75_et_100_C_5_delta_49.csv   75_et_100_C_5_delta_94.csv\n",
            "75_et_100_C_5_delta_4.csv    75_et_100_C_5_delta_95.csv\n",
            "75_et_100_C_5_delta_50.csv   75_et_100_C_5_delta_96.csv\n",
            "75_et_100_C_5_delta_51.csv   75_et_100_C_5_delta_97.csv\n",
            "75_et_100_C_5_delta_52.csv   75_et_100_C_5_delta_98.csv\n",
            "75_et_100_C_5_delta_53.csv   75_et_100_C_5_delta_99.csv\n",
            "75_et_100_C_5_delta_54.csv   75_et_100_C_5_delta_9.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTeZj2Z3eJwV",
        "colab_type": "text"
      },
      "source": [
        "Every experiment is repeated 100 times.\n",
        "\n",
        "Let's check the trace of one experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRwGpDEHeMkK",
        "colab_type": "code",
        "outputId": "316aee17-1e9f-4405-d7d7-de8001064423",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        }
      },
      "source": [
        "filename = \"BandwidthEstimationTraces/training/SingleLinkCapacity100/75_et_100_C_5_delta/75_et_100_C_5_delta_33.csv\"\n",
        "df = pd.read_csv(filename)\n",
        "df"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>100</th>\n",
              "      <th>25</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.00060</td>\n",
              "      <td>4.9907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.99982</td>\n",
              "      <td>9.9443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.00340</td>\n",
              "      <td>14.9910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.01250</td>\n",
              "      <td>19.9870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.09670</td>\n",
              "      <td>25.0280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.10830</td>\n",
              "      <td>29.9820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.27700</td>\n",
              "      <td>35.3180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.27810</td>\n",
              "      <td>39.9750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.24280</td>\n",
              "      <td>44.5390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.42630</td>\n",
              "      <td>50.0520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.44800</td>\n",
              "      <td>54.8200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1.33250</td>\n",
              "      <td>59.9640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1.39210</td>\n",
              "      <td>65.1400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1.57500</td>\n",
              "      <td>70.8650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1.50830</td>\n",
              "      <td>75.2580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1.49620</td>\n",
              "      <td>80.2280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1.68820</td>\n",
              "      <td>85.9380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1.81020</td>\n",
              "      <td>90.3930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1.76140</td>\n",
              "      <td>95.4080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1.73000</td>\n",
              "      <td>100.1100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        100        25\n",
              "0   1.00060    4.9907\n",
              "1   0.99982    9.9443\n",
              "2   1.00340   14.9910\n",
              "3   1.01250   19.9870\n",
              "4   1.09670   25.0280\n",
              "5   1.10830   29.9820\n",
              "6   1.27700   35.3180\n",
              "7   1.27810   39.9750\n",
              "8   1.24280   44.5390\n",
              "9   1.42630   50.0520\n",
              "10  1.44800   54.8200\n",
              "11  1.33250   59.9640\n",
              "12  1.39210   65.1400\n",
              "13  1.57500   70.8650\n",
              "14  1.50830   75.2580\n",
              "15  1.49620   80.2280\n",
              "16  1.68820   85.9380\n",
              "17  1.81020   90.3930\n",
              "18  1.76140   95.4080\n",
              "19  1.73000  100.1100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgpozAfk8YZv",
        "colab_type": "text"
      },
      "source": [
        "The header is not a sample. It just tells us the scenario, i.e. total channel capacity (Mbps) and available bandwith (Mbps).\n",
        "\n",
        "The columns are:\n",
        "* $g_\\text{in} / g_\\text{ack}$\n",
        "* Some sort of time stamp that we will ignore (not well described in the dataset)\n",
        "\n",
        "Let's rename the columns to avoid ambiguity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj41JABXM355",
        "colab_type": "code",
        "outputId": "af069ab5-c889-45f2-d8ba-a35ae1051bb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "df.columns=['gap_ratio', 'timestamp']\n",
        "df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gap_ratio</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.00060</td>\n",
              "      <td>4.9907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.99982</td>\n",
              "      <td>9.9443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.00340</td>\n",
              "      <td>14.9910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.01250</td>\n",
              "      <td>19.9870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.09670</td>\n",
              "      <td>25.0280</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gap_ratio  timestamp\n",
              "0    1.00060     4.9907\n",
              "1    0.99982     9.9443\n",
              "2    1.00340    14.9910\n",
              "3    1.01250    19.9870\n",
              "4    1.09670    25.0280"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv8RhycEQsDM",
        "colab_type": "text"
      },
      "source": [
        "# Feature engineering\n",
        "\n",
        "Each experiment will be a sample.\n",
        "\n",
        "The features of a sample are the elements of the histogram of the first column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFbGLRg6NfSM",
        "colab_type": "code",
        "outputId": "30a41c78-64cd-4b8f-97ee-23a7e974d339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "plt.hist(df['gap_ratio'], density=True)\n",
        "\n",
        "# density:  True garantees that the area is 1 (such that hist approximates a \n",
        "#                 probability densitplt.hist(x)y function)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2.46797799, 1.23398899, 0.6169945 , 1.23398899, 1.23398899,\n",
              "        1.23398899, 1.23398899, 0.6169945 , 0.6169945 , 1.85098349]),\n",
              " array([0.99982 , 1.080858, 1.161896, 1.242934, 1.323972, 1.40501 ,\n",
              "        1.486048, 1.567086, 1.648124, 1.729162, 1.8102  ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGmGhS1Ij/hfxNYmhR1D1zQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cncq7wY4PkDR",
        "colab_type": "text"
      },
      "source": [
        "However, the sequence of bins is chosen automatically by `matplotlib` and may change from an experiment to another.\n",
        "\n",
        "We need instead to describe all the experiments with a uniform set of features => The sequence of bins must be the same for all the experiments.\n",
        "\n",
        "To do so:\n",
        "* Open all files\n",
        "* Take the min and max gap from all the experiments\n",
        "* Divide the [min,max] interval uniformly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcGA9B5LQaSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_all_csv_files(folder):\n",
        "  \"\"\"\n",
        "  Credits to https://perials.com/getting-csv-files-directory-subdirectories-using-python/\n",
        "\n",
        "  Returns all the csv files within the folder, and all subfolders\n",
        "  \"\"\"\n",
        "  import os\n",
        "  from glob import glob\n",
        "  PATH = \"/home/someuser/projects/someproject\"\n",
        "  EXT = \"*.csv\"\n",
        "  all_csv_files = [file\n",
        "                  for path, subdir, files in os.walk(folder)\n",
        "                  for file in glob(os.path.join(path, \"*.csv\"))]\n",
        "  return all_csv_files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jpmcIZJ5yGu",
        "colab_type": "text"
      },
      "source": [
        "To avoid data leaks, we compute the max and the min on the training set only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DjjYN6d54T0",
        "colab_type": "code",
        "outputId": "38ce54a8-2b06-4ea9-9ab0-4f3b8c0e24b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_files = get_all_csv_files('BandwidthEstimationTraces/training')\n",
        "print('Found ', len(train_files), ' training files' )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found  1100  training files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laMuRhZqv6dR",
        "colab_type": "text"
      },
      "source": [
        "Find the min and max"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWFro1CZkRua",
        "colab_type": "code",
        "outputId": "7b723626-e65f-43b9-8f6a-f4c4b053f090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "min_gap = float('inf')\n",
        "max_gap = 0\n",
        "\n",
        "for filename in train_files:\n",
        "      df = pd.read_csv(filename)\n",
        "      df.columns=['gap_ratio', 'timestamp']\n",
        "      trace_min = min( df['gap_ratio'] )\n",
        "      trace_max = max( df['gap_ratio'] )\n",
        "      min_gap = min ( [ min_gap, trace_min ] )\n",
        "      max_gap = max ( [ max_gap, trace_max ] )\n",
        "\n",
        "print('min_gap:', min_gap, ' max_gap:',max_gap)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "min_gap: 0.95975  max_gap: 2.318\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS7y7cT-XmpQ",
        "colab_type": "text"
      },
      "source": [
        "Let's create the bins that we will use for all experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPnUK2tiXuEB",
        "colab_type": "code",
        "outputId": "57f8edf3-9aac-4455-e8d2-24e3fda4a301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "N = 8 # num of bins\n",
        "\n",
        "bin_size = (max_gap-min_gap)/N \n",
        "bins = [min_gap + i * bin_size for i in range(0, N+1)]\n",
        "bins"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.95975,\n",
              " 1.1295312499999999,\n",
              " 1.2993125,\n",
              " 1.4690937499999999,\n",
              " 1.638875,\n",
              " 1.8086562499999999,\n",
              " 1.9784375,\n",
              " 2.14821875,\n",
              " 2.318]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atGuyYNKYR-1",
        "colab_type": "text"
      },
      "source": [
        "Just as a visual check, let's plot again the previous histogram with these new bins"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWkFTXN0YWH2",
        "colab_type": "code",
        "outputId": "429f6c1b-3b7e-48a4-f779-122ce5130053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "filename = \"BandwidthEstimationTraces/training/SingleLinkCapacity100/75_et_100_C_5_delta/75_et_100_C_5_delta_33.csv\"\n",
        "df = pd.read_csv(filename)\n",
        "df.columns=['gap_ratio', 'timestamp']\n",
        "hist_values, bins, bars = plt.hist( df['gap_ratio'], density=True, bins=bins)\n",
        "print(hist_values )"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.76697957 0.88348978 1.17798638 0.88348978 0.88348978 0.29449659\n",
            " 0.         0.        ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyjd51oBcD8n",
        "colab_type": "text"
      },
      "source": [
        "Let's obtain the histogram for all the experiments. For each experiment, we construct a feature vector:\n",
        "\n",
        "  `[bin1_freq, bin2_freq, ...., binN_freq]`\n",
        "\n",
        "and a label `avail_bandwidth`.\n",
        "\n",
        "We write these operations in a function `construct_dataset` that we will then use for the training and the test dataset.\n",
        "\n",
        "This is to avoid **copied and pasted code**, which are error prone."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCEHvKgTaGjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_file(filename, bins):\n",
        "      \"\"\"\n",
        "      For the connection described in filename, it\n",
        "      returns a feature vector and the available bandwidth\n",
        "      \"\"\"\n",
        "      df = pd.read_csv(filename)\n",
        "\n",
        "      # Observe that for each experiment, the available bandwidth corresponds \n",
        "      # to the name of the second column\n",
        "      avail_band = df.columns[1]\n",
        "\n",
        "\n",
        "      df.columns=['gap_ratio', 'timestamp']\n",
        "\n",
        "      histogram_values, bins, bars = plt.hist( df['gap_ratio'], density=True, \n",
        "                                              bins=bins)\n",
        "\n",
        "      return histogram_values, avail_band\n",
        "\n",
        "\n",
        "\n",
        "def construct_dataset(files, bins):\n",
        "  \"\"\"\n",
        "  Build an X,y from the files\n",
        "  \"\"\"\n",
        "\n",
        "  X = np.empty((0,N), int)\n",
        "  label = []\n",
        "\n",
        "  for filename in files:\n",
        "      histogram_values, avail_band = process_file(filename, bins)\n",
        "\n",
        "      X = np.vstack(( X, histogram_values) )\n",
        "      label.append(avail_band)\n",
        "\n",
        "  return X,label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6Ekap9q-_ia",
        "colab_type": "code",
        "outputId": "21e7c26e-2579-48ef-cd79-2624c1c49099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "X_train_raw, label_train_raw = construct_dataset(train_files, bins)\n",
        "\n",
        "test_files = get_all_csv_files('BandwidthEstimationTraces/testing')\n",
        "print ('Found ', len(test_files), ' test files'  )\n",
        "\n",
        "X_test, label_test = construct_dataset(test_files, bins)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found  1000  test files\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqAEguf8BqjeukYZBs08AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STsxNvnVB-ZN",
        "colab_type": "code",
        "outputId": "fb5dba0c-a20b-4cae-c7c5-1936e79007ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "print('X_train\\n', X_train_raw[0:6, :] )\n",
        "print('\\n\\nlabel_train\\n',label_train_raw )"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train\n",
            " [[2.94496595 1.17798638 0.88348978 0.58899319 0.29449659 0.\n",
            "  0.         0.        ]\n",
            " [3.23946254 0.88348978 1.17798638 0.58899319 0.         0.\n",
            "  0.         0.        ]\n",
            " [2.65046935 1.76697957 0.88348978 0.29449659 0.29449659 0.\n",
            "  0.         0.        ]\n",
            " [2.94496595 1.17798638 1.47248297 0.         0.29449659 0.\n",
            "  0.         0.        ]\n",
            " [2.94496595 1.17798638 0.88348978 0.58899319 0.29449659 0.\n",
            "  0.         0.        ]\n",
            " [2.65046935 1.76697957 0.88348978 0.58899319 0.         0.\n",
            "  0.         0.        ]]\n",
            "\n",
            "\n",
            "label_train\n",
            " ['25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '37.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '12.5', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '25', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '50', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75', '75']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwqgnS9lKLFp",
        "colab_type": "text"
      },
      "source": [
        "### Class imbalance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03Tclu74Kbi5",
        "colab_type": "text"
      },
      "source": [
        "Let's check for class imbalance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugBoY1tB8gG9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aac394b0-2e2a-45f9-a91d-034def76cb5d"
      },
      "source": [
        "Counter(label_train_raw)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'12.5': 100, '25': 400, '37.5': 100, '50': 400, '75': 100})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJkz2kra9rDP",
        "colab_type": "text"
      },
      "source": [
        "Correct using Synthetic Minority Over-Sampling Technique (SMOTE) (see `03.classification.ipynb`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toS5Tgne-lnN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "f47c0003-bec3-41d0-8bb3-287a7c280cf7"
      },
      "source": [
        "smote = SMOTE()\n",
        "X_train, label_train = smote.fit_sample(X_train_raw, label_train_raw)\n",
        "\n",
        "Counter(label_train)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'12.5': 400, '25': 400, '37.5': 400, '50': 400, '75': 400})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K03VxUJ1idJp",
        "colab_type": "text"
      },
      "source": [
        "### Scaling\n",
        "With NN is important to **scale** the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CpWt6qKivif",
        "colab_type": "code",
        "outputId": "ec509122-e55d-4433-ca13-733f08889a0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print('X_train scaled\\n', X_train[0:6, :] )"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train scaled\n",
            " [[0.46666667 0.5        0.42857143 0.33333333 0.16666667 0.\n",
            "  0.         0.        ]\n",
            " [0.53333333 0.375      0.57142857 0.33333333 0.         0.\n",
            "  0.         0.        ]\n",
            " [0.4        0.75       0.42857143 0.16666667 0.16666667 0.\n",
            "  0.         0.        ]\n",
            " [0.46666667 0.5        0.71428571 0.         0.16666667 0.\n",
            "  0.         0.        ]\n",
            " [0.46666667 0.5        0.42857143 0.33333333 0.16666667 0.\n",
            "  0.         0.        ]\n",
            " [0.4        0.75       0.42857143 0.33333333 0.         0.\n",
            "  0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAY-VQMf80Nf",
        "colab_type": "text"
      },
      "source": [
        "### One-hot encoding\n",
        "**One-hot encode** the target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZWAb6fb_lMv",
        "colab_type": "code",
        "outputId": "8aff4746-2932-4fcd-c135-8f9ef00520da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "# OneHotEncoder works best with dataframes. Let's convert our y lists to \n",
        "# dataframes\n",
        "label_train_df = pd.DataFrame({'label':label_train})\n",
        "label_test_df = pd.DataFrame({'label':label_test})\n",
        "\n",
        "label_train_df"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     label\n",
              "0       25\n",
              "1       25\n",
              "2       25\n",
              "3       25\n",
              "4       25\n",
              "...    ...\n",
              "1995    75\n",
              "1996    75\n",
              "1997    75\n",
              "1998    75\n",
              "1999    75\n",
              "\n",
              "[2000 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_iC8vrtk3uE",
        "colab_type": "code",
        "outputId": "9e5ade16-9261-43f3-ea5e-d4cdd76b1689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "one_hot_encoder = ce.OneHotEncoder(cols=['label'], use_cat_names='True')\n",
        "one_hot_encoder.fit(label_train_df)\n",
        "y_train = one_hot_encoder.transform(label_train_df)\n",
        "y_test  = one_hot_encoder.transform(label_test_df)\n",
        "\n",
        "print('y_train', y_train)\n",
        "\n",
        "num_of_classes = y_train.shape[1]\n",
        "class_names = y_train.columns\n",
        "print('There are ', num_of_classes, ' classes:', class_names)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_train       label_25  label_37.5  label_12.5  label_50  label_75\n",
            "0            1           0           0         0         0\n",
            "1            1           0           0         0         0\n",
            "2            1           0           0         0         0\n",
            "3            1           0           0         0         0\n",
            "4            1           0           0         0         0\n",
            "...        ...         ...         ...       ...       ...\n",
            "1995         0           0           0         0         1\n",
            "1996         0           0           0         0         1\n",
            "1997         0           0           0         0         1\n",
            "1998         0           0           0         0         1\n",
            "1999         0           0           0         0         1\n",
            "\n",
            "[2000 rows x 5 columns]\n",
            "There are  5  classes: Index(['label_25', 'label_37.5', 'label_12.5', 'label_50', 'label_75'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsDxd9KuCnzg",
        "colab_type": "text"
      },
      "source": [
        "# Build a NN model\n",
        "\n",
        "To train faster, change the runtime to GPU.\n",
        "\n",
        "Now, let's build a NN architecture. The size of each sample is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLRGAHB2oPJJ",
        "colab_type": "code",
        "outputId": "975ba346-51ec-4289-d288-ff9673529e4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_size = X_train.shape[1]\n",
        "print('The sample size is ', sample_size, \n",
        "      ', which should correspond to the number of bins ', N)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sample size is  8 , which should correspond to the number of bins  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9aa9XrBoFAm",
        "colab_type": "text"
      },
      "source": [
        "Let's fix the learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEIxDxPXoKNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_rate = 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PTmksjnoM3k",
        "colab_type": "text"
      },
      "source": [
        "Let's write a function to build a model. We need to:\n",
        "* Define the architecture\n",
        "* Compile it, i.e., decide the loss function to minimize, the optimization function and the metrics to show"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0zcfeTEC0KF",
        "colab_type": "code",
        "outputId": "5f6f8708-dea4-4751-a06e-a4b7ab74ceab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "def build_model():\n",
        "  # Reproducibility: to ensure every time we generate a model, its weights are \n",
        "  #     always initialized in the same way\n",
        "  # To know more: \n",
        "  #       https://machinelearningmastery.com/reproducible-results-neural-networks-keras/\n",
        "  np.random.seed(6)\n",
        "  tf.set_random_seed(4)\n",
        "\n",
        "\n",
        "\n",
        "  model = Sequential([\n",
        "    Dense(200, input_dim=sample_size, activation='relu' ),\n",
        "    Dense(50, input_dim=sample_size, activation='relu' ),\n",
        "    Dense(30, input_dim=sample_size, activation='relu' ),\n",
        "    Dense(15, input_dim=sample_size, activation='relu' ),\n",
        "    Dense(8, input_dim=sample_size, activation='relu' ),\n",
        "    Dense(num_of_classes, activation=\"softmax\")\n",
        "  ])\n",
        "  # The first layer has 4 neurons and take 8 input values.\n",
        "  # In the first layer, you always need to specify the input_dim\n",
        "  #\n",
        "  # Note that each layer adds implicitly a bias term (we do not need to care about \n",
        "  # it)\n",
        "  #\n",
        "  # The last layer is a softmax, since we are doing classification\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  model.compile(loss=\"categorical_crossentropy\", \n",
        "              optimizer=keras.optimizers.SGD(lr=learn_rate) ,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-ffe63596b051>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    Dense(50, input_dim=sample_size, activation='relu' ),\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0SfFwbExxs9",
        "colab_type": "text"
      },
      "source": [
        "See Ch.10 of [Ge19] to know more about these hyper-parameters.\n",
        "\n",
        "Loss function:\n",
        "* `categorical_crossentropy` is the cross-entropy as we defined in the slides\n",
        "* In binary classification then use `sigmoid` activation function in the output layer instead of the `softmax` and `binary_crossentropy` loss.\n",
        "\n",
        "Optimizer:\n",
        "* We are using Stochastic Gradient Descent with learning rate $\\eta=$ `learn_rate`\n",
        "\n",
        "Metrics:\n",
        "* We are asking Keras to show at each epoch the accuracy. Note that the accuracy value is ignored during training. This metric is just visualized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VugALgbowkG",
        "colab_type": "text"
      },
      "source": [
        "Let's build a model and print its description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0eA8qTowf16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model()\n",
        "model.summary()\n",
        "plot_model(model)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AZY1xtwl0jq",
        "colab_type": "text"
      },
      "source": [
        "# Training and testing\n",
        "\n",
        "To train faster, change the runtime to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gFX8S31pjS8",
        "colab_type": "text"
      },
      "source": [
        "### Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAPBL60-pddu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xebwBVml9sSw",
        "colab_type": "text"
      },
      "source": [
        "We divide the training set in two subsets:\n",
        "* The training subset\n",
        "* The validation subset \n",
        "\n",
        "Keras:\n",
        "* Minimizes the loss on the training subset\n",
        "* The validation subset is ignored => No impact on training\n",
        "* The validation subset is just used to check overfitting:\n",
        "    * If validation loss starts to increase\n",
        "\n",
        "\n",
        "\n",
        "Training just takes one line of code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBRB-Ofm6gWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(label_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwDQU9pa6sjC",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Note** Keras divides the trainining set in two consecutive parts ==> Need to shuffle data first\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_Y4xLk75lH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, y_train, label_train = shuffle(X_train, y_train,label_train, \n",
        "                                        random_state=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je9WQ_Si-Xt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(label_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToEr6OsEpvi7",
        "colab_type": "text"
      },
      "source": [
        "We can add some useful operations to training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jIuLvmiq4rt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(nn_file, X_tr, y_tr, epochs, overwrite=False):\n",
        "  \"\"\"\n",
        "  nn_file:  Before training, the model contained in this file will be loaded\n",
        "            After training, the resulting model will be written in this file\n",
        "\n",
        "  \n",
        "  overwrite: If true, the model will be built and trained from scratch\n",
        "  \"\"\"\n",
        "  \n",
        "  ##################\n",
        "  #### CALLBACKS ###\n",
        "  ##################\n",
        "  # These functions are called at every epoch\n",
        "  plot_cb = PlotLossesCallback()  # Plots the loss\n",
        "  checkpoint_cb = ModelCheckpoint(nn_file) # Stores weights\n",
        "  logger_cb = CSVLogger(nn_file+'.csv', append=True) # Stores history\n",
        "                # see https://theailearner.com/2019/07/23/keras-callbacks-csvlogger/\n",
        "\n",
        "\n",
        "\n",
        "  if overwrite==True:\n",
        "    os.remove(nn_file)\n",
        "    os.remove(nn_file+'.csv')\n",
        "\n",
        "\n",
        "  if not isfile(nn_file):\n",
        "    model = build_model()\n",
        "  else:\n",
        "    model = load_model(nn_file)\n",
        "\n",
        "  history = model.fit(X_tr, y_tr, epochs=epochs, \n",
        "                      callbacks = [plot_cb, checkpoint_cb, logger_cb], \n",
        "                      validation_split=0.2 )\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAqp2x0EvexH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = train_model(nn_file, X_train, y_train, epochs=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtToyIGtxY83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.read_csv(nn_file+'.csv').head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ADD1Y_nxCcv",
        "colab_type": "text"
      },
      "source": [
        "Write a function to plot the training history"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVY854n8wSko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_train_hist(csv_hist_file):\n",
        "  hist_df = pd.read_csv(csv_hist_file)\n",
        "  hist_df[['acc', 'loss', 'val_acc', 'val_loss']].plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc5dztcSxnQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_train_hist(nn_file+'.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PsMSdYox7Gz",
        "colab_type": "text"
      },
      "source": [
        "Let's continue training the same model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z483MReLyOwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = train_model(nn_file, X_train, y_train, epochs=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWJCpnySyizx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_train_hist(nn_file+'.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQcQJQ0LyplW",
        "colab_type": "text"
      },
      "source": [
        "Observe that we have the entire history: the previous training and the latter toghether"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2u6u_E06zk4",
        "colab_type": "text"
      },
      "source": [
        "Let's train the model with more epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRAvnAOY62Ou",
        "colab_type": "code",
        "colab": {},
        "cellView": "code"
      },
      "source": [
        "model = train_model(nn_file, X_train, y_train, epochs=5000)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IADgsSbyEcUh",
        "colab_type": "text"
      },
      "source": [
        "Let's use the model for prediction. With next lines you get the probability of each sample being in a class and the predicted class (the one with the highest probability)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbPM7RCNzuzq",
        "colab_type": "text"
      },
      "source": [
        "# Arrivato qua"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TG8TIN4zsKw",
        "colab_type": "text"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvWVnm5pG505",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_proba = model.predict(X_test)\n",
        "print( 'y_proba\\n', y_proba)\n",
        "\n",
        "pred_label = model.predict_classes(X_test)\n",
        "print( 'pred_label\\n', pred_label )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRDgFle0ByJ5",
        "colab_type": "text"
      },
      "source": [
        "The best metric is the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9OReizvEXT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HymnF9ty7RGX",
        "colab_type": "text"
      },
      "source": [
        "We need to convert one-hot-encoded values to numerical labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExiitzPmEDyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "true_label = np.argmax(y_test.values, axis=1)\n",
        "true_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtUXq83bB055",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_conf_mat(true_label, pred_label, np.array(class_names) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXPP66zQVXGF",
        "colab_type": "text"
      },
      "source": [
        "Some class does not appear in the matrix ==> no such class in the test set nor among the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WlfFdI0VoZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_labels(true_label, pred_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqsxRULREmZv",
        "colab_type": "text"
      },
      "source": [
        "If there are `nan`, it means that there is a label that does not appear in the test set but it is predicted by our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30txCPSGBhTB",
        "colab_type": "text"
      },
      "source": [
        "Let's train again our NN.\n",
        "\n",
        "**Note** If you directly run `model.fit(..)`, you will start with the weigthts of the last training:\n",
        "* This is convenient as you can progressively improve your model\n",
        "* However, since we want to contrast different training scenarios, we will start every time from scratch (re-building the network every time)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8ix3cJZCBUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential([\n",
        "  Dense(50, input_dim=sample_size, activation='relu' ),\n",
        "  Dense(30, input_dim=sample_size, activation='relu' ),\n",
        "  Dense(15, input_dim=sample_size, activation='relu' ),\n",
        "  Dense(8, input_dim=sample_size, activation='relu' ),\n",
        "  Dense(num_of_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", \n",
        "              optimizer=keras.optimizers.SGD(lr=learn_rate) ,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "plot_losses = PlotLossesCallback()\n",
        "\n",
        "model.fit(X_train_bal, y_train_bal, epochs=epochs, \n",
        "                    callbacks = [plot_losses], validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI2udREmD-G1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_label = model.predict_classes(X_test_scaled)\n",
        "plot_conf_mat(true_label, pred_label, np.array(class_names) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjC05GoUEFZa",
        "colab_type": "text"
      },
      "source": [
        "We still need to improve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUTc4Xrx9x11",
        "colab_type": "text"
      },
      "source": [
        "### Automate build"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPEVn53q9J5y",
        "colab_type": "text"
      },
      "source": [
        "Instead of writing every time the same code, we wrap it into a function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR0eiQu391YP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  # Reproducibility: to ensure every time we generate a model, its weights are \n",
        "  #     always initialized in the same way\n",
        "  # To know more: \n",
        "  #       https://machinelearningmastery.com/reproducible-results-neural-networks-keras/\n",
        "  np.random.seed(6)\n",
        "  tf.set_random_seed(4)\n",
        "\n",
        "\n",
        "  model = Sequential([\n",
        "    Dense(50, input_dim=sample_size, activation='relu' ),\n",
        "    Dense(30, input_dim=sample_size, activation='relu' ),\n",
        "    Dense(15, input_dim=sample_size, activation='relu' ),\n",
        "    Dense(8, input_dim=sample_size, activation='relu' ),\n",
        "    Dense(num_of_classes, activation=\"softmax\")\n",
        "  ])\n",
        "\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=keras.optimizers.SGD(lr=learn_rate),\n",
        "                metrics=['accuracy']\n",
        "                )\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t0t-suhaEu2f"
      },
      "source": [
        "Check if the model improves with more epochs. \n",
        "\n",
        "We now also store the model in Google Drive, to reuse it another day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPT7aAEQ0d_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs=270\n",
        "\n",
        "plot_losses = PlotLossesCallback() \n",
        "nn_file = my_path + 'nn1.h5'\n",
        "model = build_model()\n",
        "history = model.fit(X_train_bal, y_train_bal, epochs=epochs, \n",
        "                    callbacks = [plot_losses], validation_split=0.2 )\n",
        "  \n",
        "# Save the trained model\n",
        "model.save( nn_file )\n",
        "\n",
        "# Save the history as well\n",
        "pd.DataFrame.from_dict(history.history).to_csv(nn_file+'.hist.csv', \n",
        "                                                 index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYQiYvq2Hl-r",
        "colab_type": "text"
      },
      "source": [
        "**OVERFITTING**!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqmGKVf0FZdL",
        "colab_type": "text"
      },
      "source": [
        "Before tackling overfitting, if you run this notebook many times, the following function avoids useless computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPQV5g4xvssJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_or_load_model(nn_file, X_tr, y_tr, model=None):\n",
        "  plot_losses = PlotLossesCallback()  # Just to plot the evolution of \n",
        "                                    # loss during training\n",
        "\n",
        "  if not isfile(nn_file):\n",
        "    if model==None:\n",
        "      model = build_model()\n",
        "    # else, we do not build it from scratch, we just continue training\n",
        "    #       the model in input\n",
        "\n",
        "    history = model.fit(X_tr, y_tr, epochs=epochs, \n",
        "                      callbacks = [plot_losses], validation_split=0.2 )\n",
        "    model.save( nn_file )\n",
        "\n",
        "    # From https://stackoverflow.com/a/59854096/2110769\n",
        "    pd.DataFrame.from_dict(history.history).to_csv(nn_file+'.hist.csv',\n",
        "                                                   index=False)\n",
        "    return model\n",
        "    \n",
        "  else:\n",
        "    model = load_model( nn_file )\n",
        "    hist_df = pd.read_csv(nn_file+'.hist.csv')\n",
        "    hist_df.plot()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxO4wqr5FMQO",
        "colab_type": "text"
      },
      "source": [
        "Let's run this function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxP6x7ZXFzd5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn_file = my_path + 'nn1.h5'\n",
        "\n",
        "model = train_or_load_model(nn_file, X_train_bal, y_train_bal)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4XbrlG0Ta-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_label = model.predict_classes(X_test_scaled)\n",
        "plot_conf_mat(true_label, pred_label, np.array(class_names) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fPMkJSexgSY",
        "colab_type": "text"
      },
      "source": [
        "# Solving overfitting\n",
        "\n",
        "Three solutions: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToyJ5gpvIM3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_or_load_model(nn_file, X_tr, y_tr):\n",
        "  plot_losses = PlotLossesCallback()  # Just to plot the evolution of \n",
        "                                    # loss during training\n",
        "\n",
        "  if not isfile(nn_file):\n",
        "    model = build_model()\n",
        "\n",
        "    history = model.fit(X_tr, y_tr, epochs=epochs, \n",
        "                        callbacks = [plot_losses],\n",
        "                        validation_split = 0.1 # I added this line\n",
        "                        )\n",
        "    \n",
        "    model.save( nn_file )\n",
        "\n",
        "    # From https://stackoverflow.com/a/59854096/2110769\n",
        "    pd.DataFrame.from_dict(history.history).to_csv(nn_file+'.hist.csv',\n",
        "                                                   index=False)\n",
        "    return model\n",
        "    \n",
        "  else:\n",
        "    model = load_model( nn_file )\n",
        "    hist_df = pd.read_csv(nn_file+'.hist.csv')\n",
        "    hist_df.plot()\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hlfm6hS1yiC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn_file = my_path + 'nn3.h5'\n",
        "model = train_or_load_model(nn_file, X_train_scaled_sh, y_train_sh)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxqZWIuqz1AI",
        "colab_type": "text"
      },
      "source": [
        "It is normal that the model performs worse on the validation set. \n",
        "\n",
        "Overfitting shows up when the validation loss increases with the epochs <= the model is adapting too much on the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u0cMK1JDKLH",
        "colab_type": "text"
      },
      "source": [
        "# Limitation of the work\n",
        "\n",
        "* Only a finite set of available bandwidth values are used (25, 50, 75 Mbps). In reality, any value can occur => Need to extend the test and validation test with random bandwidth values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO57soSj0vmj",
        "colab_type": "text"
      },
      "source": [
        "# Regression\n",
        "\n",
        "To perform regression with neural network, you do the same things, just change the last layer: instead of `softmax`, it is just `Dense(1)` (see pagg. 307-308 of [Ge19])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VYbiDvHxLtU",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "\n",
        "* [Ge19] Geron, Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2019, O'Reilly\n",
        "* [KhConf19] Khangura, S. K. (2019). Neural Network-based Available Bandwidth Estimation from TCP Sender-side Measurements. In IEEE/IFIP PEMWN.\n",
        "* [KhThesis19] Khangura, S. K. (2019). Machine Learning-based Available Bandwidth Estimation. Leibniz University."
      ]
    }
  ]
}